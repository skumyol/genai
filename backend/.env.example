# LLM Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here

# LLM Fallback Configuration (optional)
# Format: "provider1:model1,provider2:model2"
# Example with multiple fallback models (small/fast first):
LLM_FALLBACK_MODELS=openrouter:meta-llama/llama-3.2-3b-instruct:free,openrouter:meta-llama/llama-3-8b-instruct:free,openrouter:openai/gpt-oss-20b:free,openrouter:meta-llama/llama-3.3-70b-instruct:free,openrouter:deepseek/deepseek-r1-distill-llama-70b:free,local:llama3.2

# LLM Request Settings
LLM_REQUEST_TIMEOUT_SECONDS=60
LLM_VERBOSE=0
LLM_VERBOSE_FULL=0
## Optional: override local LLM endpoints (comma-separated)
# LLM_LOCAL_ENDPOINTS=http://localhost:11434/api/chat,http://127.0.0.1:11434/api/chat

# Optional (recommended for OpenRouter free-tier reliability)
# OPENROUTER_REFERRER=http://localhost:5173
# LLM_APP_NAME=GenAI Game

# Example configurations for different scenarios:
# High reliability (multiple OpenRouter models):
# LLM_FALLBACK_MODELS=openrouter:meta-llama/llama-3.3-70b-instruct:free,openrouter:meta-llama/llama-3.2-3b-instruct:free,openrouter:openai/gpt-oss-20b:free

# Local-first with cloud backup:
# LLM_FALLBACK_MODELS=local:llama3.2,local:mistral,openrouter:meta-llama/llama-3.3-70b-instruct:free

# Testing environment:
# LLM_FALLBACK_MODELS=test:mock

# Memory Summarization (optional overrides)
# Provider/model for NPC dialogue summarization. If unset, defaults to openrouter + a free llama model.
# Example providers: openrouter, local, test
MEMORY_SUMMARY_PROVIDER=openrouter
MEMORY_SUMMARY_MODEL=meta-llama/llama-3.2-3b-instruct:free
# Target character budget for generated summaries
MEMORY_SUMMARY_MAX_CHARS=2000
